{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Masking\n",
    "from sklearn.cross_validation import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkEmbedding(fp, dictionary=None, fn='glove.8B.300d.txt'):\n",
    "    embeddings_index = {}\n",
    "    embeddings_matrix = []\n",
    "    f = open(os.path.join(fp, fn))\n",
    "    for (i, line)in enumerate (f):\n",
    "        values = line.split()\n",
    "        if len(values) > 301:\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coefs = np.asarray(values[-300:] , dtype='float32')\n",
    "            except Exception as e:\n",
    "                print (word, values[1:])\n",
    "                raise e\n",
    "        else:\n",
    "            coefs = np.asarray(values[1:] , dtype='float32')\n",
    "            word = values[0]\n",
    "        if dictionary:\n",
    "            if word in dictionary:\n",
    "                ii = len(embeddings_index.keys())\n",
    "                embeddings_index[word] = ii # FIXME: this needs to only increment if the dict clause is triggered\n",
    "                embeddings_matrix.append(coefs)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            embeddings_index[word] = i\n",
    "            embeddings_matrix.append(coefs)\n",
    "    f.close()\n",
    "    embeddings_matrix = np.concatenate(embeddings_matrix, axis=0)\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    embedding_layer = Embedding(len(embeddings_index),\n",
    "                            300,\n",
    "                            weights=[embeddings_matrix],\n",
    "                            input_length=1000,\n",
    "                            trainable=False)\n",
    "    return (embedding_layer,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "(eml, emi) = mkEmbedding(\"/Users/timpierson/glove.6B/\",\"glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator(embeddingLookup=emi, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(sum(sum(dg.paragraphs,[]), []) + [\"unknown\", \"~\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4006 word vectors.\n"
     ]
    }
   ],
   "source": [
    "(eml, emi) = mkEmbedding(\"/Users/timpierson/glove.6B/\", dictionary=vocab, fn=\"glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator(embeddingLookup=emi, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that we can go from embeddings back to text.  Note this will return repeated \"padding\" values.\n",
    "# x, y, m =dg[1]\n",
    "# t = y.argmax(axis=2).flatten()\n",
    "# [dg.lookupEmbedding[x] for x in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[3531,   13, 1571,  105,   52, 2936],\n",
       "         [  42,   12,   45,  310,    1,    0],\n",
       "         [ 380,   12,    3,  429, 1290,    1],\n",
       "         ...,\n",
       "         [   0,    0,    0,    0,    0,    0],\n",
       "         [   0,    0,    0,    0,    0,    0],\n",
       "         [   0,    0,    0,    0,    0,    0]]),\n",
       "  array([[   0,    0,    0,    0,    0,    0],\n",
       "         [1842, 3531,   13, 1571,  105,   52],\n",
       "         [2936,   42,   12,   45,  310,    1],\n",
       "         ...,\n",
       "         [   0,    0,    0,    0,    0,    0],\n",
       "         [   0,    0,    0,    0,    0,    0],\n",
       "         [   0,    0,    0,    0,    0,    0]])],\n",
       " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens = len(emi.keys())\n",
    "num_encoder_tokens = num_decoder_tokens\n",
    "latent_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 34s 622ms/step - loss: 3.3421\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 27s 505ms/step - loss: 2.7701\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 29s 535ms/step - loss: 2.7064\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 28s 517ms/step - loss: 2.7659\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 29s 532ms/step - loss: 2.7425\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 27s 508ms/step - loss: 2.7592\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 28s 518ms/step - loss: 2.7424\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 27s 503ms/step - loss: 2.7565\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 28s 522ms/step - loss: 2.7032\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 29s 533ms/step - loss: 2.6068\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 29s 543ms/step - loss: 2.6304\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 28s 518ms/step - loss: 2.6571\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 28s 526ms/step - loss: 2.6316\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 29s 531ms/step - loss: 2.5635\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 29s 531ms/step - loss: 2.6202\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 29s 531ms/step - loss: 2.5878\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 28s 518ms/step - loss: 2.6529\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 28s 517ms/step - loss: 2.6150\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 28s 512ms/step - loss: 2.6330\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 29s 532ms/step - loss: 2.5678\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 28s 518ms/step - loss: 2.6652\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 28s 516ms/step - loss: 2.6144\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 28s 524ms/step - loss: 2.4798\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 28s 523ms/step - loss: 2.5399\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 29s 528ms/step - loss: 2.4827\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 28s 515ms/step - loss: 2.4612\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 28s 526ms/step - loss: 2.4452\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 28s 519ms/step - loss: 2.4506\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 28s 521ms/step - loss: 2.4774\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 28s 527ms/step - loss: 2.3961\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 27s 506ms/step - loss: 2.4580\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 28s 519ms/step - loss: 2.4086\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 28s 519ms/step - loss: 2.3524\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 29s 536ms/step - loss: 2.4031\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 27s 509ms/step - loss: 2.3545\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 29s 534ms/step - loss: 2.2629\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 28s 512ms/step - loss: 2.3155\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 28s 521ms/step - loss: 2.3342\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 28s 519ms/step - loss: 2.3769\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 28s 525ms/step - loss: 2.2724\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 29s 544ms/step - loss: 2.1486\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 29s 533ms/step - loss: 2.3087\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 28s 516ms/step - loss: 2.2330\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 29s 529ms/step - loss: 2.1802\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 28s 518ms/step - loss: 2.1757\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 26s 479ms/step - loss: 2.1979\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 25s 466ms/step - loss: 2.1174\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 26s 483ms/step - loss: 2.0703\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 26s 482ms/step - loss: 2.1219\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 26s 477ms/step - loss: 2.0632\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 25s 454ms/step - loss: 2.1628\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 24s 453ms/step - loss: 2.1094\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 26s 474ms/step - loss: 2.0255\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 24s 447ms/step - loss: 2.0695\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 25s 467ms/step - loss: 2.0266\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 25s 456ms/step - loss: 1.9792\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 25s 469ms/step - loss: 1.9467\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 25s 469ms/step - loss: 1.9994\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 26s 475ms/step - loss: 1.7919\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 25s 467ms/step - loss: 1.9250\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 26s 480ms/step - loss: 1.9485\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 25s 472ms/step - loss: 1.8517\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 26s 482ms/step - loss: 1.8525\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 26s 481ms/step - loss: 1.8711\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 26s 477ms/step - loss: 1.7828\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 25s 471ms/step - loss: 1.7440\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 26s 474ms/step - loss: 1.8413\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 26s 480ms/step - loss: 1.7088\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 26s 485ms/step - loss: 1.6677\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 25s 470ms/step - loss: 1.7695\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 26s 473ms/step - loss: 1.6418\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 26s 481ms/step - loss: 1.6438\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 25s 464ms/step - loss: 1.6730\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 25s 463ms/step - loss: 1.7142\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 25s 465ms/step - loss: 1.6924\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 25s 468ms/step - loss: 1.7232\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 26s 473ms/step - loss: 1.6162\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 26s 475ms/step - loss: 1.6173\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 26s 481ms/step - loss: 1.6621\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 26s 486ms/step - loss: 1.6549\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 25s 470ms/step - loss: 1.6579\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 25s 471ms/step - loss: 1.6259\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 26s 480ms/step - loss: 1.4088\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 25s 464ms/step - loss: 1.4961\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 26s 472ms/step - loss: 1.5270\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 26s 478ms/step - loss: 1.4454\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 25s 461ms/step - loss: 1.5506\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 26s 472ms/step - loss: 1.5582\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 26s 484ms/step - loss: 1.4937\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 25s 469ms/step - loss: 1.4564\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 26s 477ms/step - loss: 1.3441\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 26s 477ms/step - loss: 1.5215\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 26s 478ms/step - loss: 1.3745\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 25s 472ms/step - loss: 1.3162\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 25s 455ms/step - loss: 1.5599\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 26s 475ms/step - loss: 1.2599\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 25s 466ms/step - loss: 1.2887\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 26s 485ms/step - loss: 1.3280\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 26s 478ms/step - loss: 1.3556\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 25s 470ms/step - loss: 1.2369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x130c307f0>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,), name=\"EncoderInputs\")\n",
    "mencoder_inputs = Masking(input_shape=(None,))(encoder_inputs)\n",
    "# target_mask = Input(shape=(None,), name=\"TargetMask\")\n",
    "x = Embedding(num_encoder_tokens, latent_dim)(mencoder_inputs)\n",
    "x, state_h, state_c = LSTM(latent_dim,\n",
    "                           return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,), name=\"DecoderInputs\")\n",
    "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n",
    "\n",
    "\n",
    "# FIXME: need to deal with start character and decoder inputs.\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "## TODO: defin x entropy here and zero padded targets.\n",
    "# Compile & run training\n",
    "model.compile(optimizer='rmsprop', loss=categorical_crossentropy)\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "model.fit_generator(generator=dg,\n",
    "          epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f134ae56ff13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoder_state_input_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder_state_input_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_states' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
